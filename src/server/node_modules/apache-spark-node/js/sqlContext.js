"use strict";

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

function _toConsumableArray(arr) { if (Array.isArray(arr)) { for (var i = 0, arr2 = Array(arr.length); i < arr.length; i++) { arr2[i] = arr[i]; } return arr2; } else { return Array.from(arr); } }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

var DataFrameReader = require("./DataFrameReader");
var DataFrame = require("./DataFrame");
var java = require("./java");
var sqlc;

/**
 * The entry point for working with structured data (rows and columns) in Spark.  Allows the
 * creation of {@link DataFrame} objects as well as the execution of SQL queries.
 *
 */

var SQLContext = function () {

    /**
     * **Note:** Do not use directly. Access via {@link sqlContext}.
     */
    function SQLContext(jvm_sqlContext, jvm_javaSparkContext) {
        _classCallCheck(this, SQLContext);

        this.jvm_obj = jvm_sqlContext;
        this.jvm_jsc = jvm_javaSparkContext;
        this.jvm_DataFrame = java.import("org.apache.spark.sql.DataFrame");
    }

    /**
     * Returns a {@link DataFrame} with no rows or columns.
     *
     * @since 1.3.0
     */


    _createClass(SQLContext, [{
        key: "emptyDataFrame",
        value: function emptyDataFrame() {
            return new DataFrame(this.jvm_obj.emptyDataFrame());
        }

        /**
         * Returns a {@link DataFrameReader} that can be used to read data in as a {@link DataFrame}.
         *
         * @since 1.4.0
         */

    }, {
        key: "read",
        value: function read() {
            return new DataFrameReader(this);
        }

        /**
         * Creates a {@link DataFrame} with a single {@link LongType} column named `id`,
         * containing elements in a range with step value 1. If end is provided, the
         * range is from `start_or_end` to `end`. Otherwise it is from `0` to
         * `start_or_end`.
         *
         * @param start_or_end Start (if end is provided) or end of range.
         * @param [end=null] End of range.
         * @param [step=1] Step.
         * @since 1.4.1
         */

    }, {
        key: "range",
        value: function range(start_or_end /*: Number */) /*: Number */ /*: DataFrame */{
            var end = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;
            var step = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;

            var start, par;
            if (end === null) {
                end = start_or_end;
                start = 0;
            } else {
                start = start_or_end;
            }

            if (step === 1) {
                return new DataFrame(this.jvm_obj.range(start, end));
            } else {
                par = this.jvm_obj.sparkContext().defaultParallelism();
                return new DataFrame(this.jvm_obj.range(start, end, step, par));
            }
        }

        /**
         * Executes a SQL query using Spark, returning the result as a {@link
         * DataFrame}. The dialect that is used for SQL parsing can be configured
         * with 'spark.sql.dialect'.
         *
         * @param sqlText SQL query.
         * @since 1.3.0
         */

    }, {
        key: "sql",
        value: function sql(sqlText /*: String*/) /*: DataFrame*/{
            var logicalPlan = this.jvm_obj.parseSql(sqlText);
            var jvm_df = new this.jvm_DataFrame(this.jvm_obj, logicalPlan);
            return new DataFrame(jvm_df);
        }

        /**
         * Creates a DataFrame from an array of rows, represented as javascript
         * objects. These objects should be serializable and deserializable
         * to/from json. This function goes through the input once to determine
         * the input schema.
         *
         * @param jsonArray Array of JSON objects.
         * @since 1.3.0
         */

    }, {
        key: "createDataFrame",
        value: function createDataFrame(jsonArray /*: Array[Object] */) /*: DataFrame*/{
            var Arrays = java.import("java.util.Arrays");
            var strings = jsonArray.map(JSON.stringify);
            var jvm_List = Arrays.asList.apply(Arrays, _toConsumableArray(strings));
            var jvm_javaStringRDD = this.jvm_jsc.parallelize(jvm_List);

            return this.read().jsonRDD_(jvm_javaStringRDD);
        }
    }]);

    return SQLContext;
}();

function sqlContext(sc, jsc) {
    if (sqlc) return sqlc;

    var jvm_SQLContext = java.import("org.apache.spark.sql.SQLContext");

    var jvm_obj = new jvm_SQLContext(sc);

    sqlc = new SQLContext(jvm_obj, jsc);
    return sqlc;
}

module.exports = sqlContext;